# -*- coding: utf-8 -*-
"""Sound analysis

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Bp8O-Lst2ale5vtOe0mC3D8gvOyf5e2G
"""

#https://ai.googleblog.com/2019/07/parrotron-new-research-into-improving.html
#https://devmesh.intel.com/projects/speech-assistance-using-artificial-neural-netowrk
import librosa
# import librosa.display
# import matplotlib.pyplot as plt
import numpy as np
from dotenv import load_dotenv
# from IPython.display import Audio
from pydub import AudioSegment
import io
# Imports the Google Cloud client library
from google.cloud import speech
from google.cloud.speech import enums
from google.cloud.speech import types
from google.cloud import texttospeech

load_dotenv()

# from google.colab import drive
# drive.mount('/content/drive')

# !curl -o synth.wav https://google.github.io/tacotron/publications/parrotron/audio/blog/moon_earth_app.wav
# !curl -o impediment.wav https://google.github.io/tacotron/publications/parrotron/audio/blog/moon_earth.wav

# def plot_chroma(file_path):
#   y, sr = librosa.load(file_path)
#   plt.figure(figsize=(20, 10))
#   C = librosa.feature.chroma_cqt(y=y, sr=sr)
#   librosa.display.specshow(C, y_axis='chroma')
#   plt.colorbar()
#   plt.title('Chromagram')
  
# plot_chroma('./drive/My Drive/sounds/synth.wav')

# plot_chroma('./drive/My Drive/sounds/impediment.wav')

def gen_chroma_stft(file_path):
  y, sr = librosa.load(file_path)
  return librosa.feature.chroma_stft(y, sr)

def clean_soundfile(file_path):
  stereo_to_mono(file_path)
  y, sr = librosa.load(file_path)
  sf.write(file_path, y, sr, subtype='PCM_16')

# gen_chroma_stft('./drive/My Drive/sounds/synth.wav')

# gen_chroma_stft(IMPEDIMENT_FILE)

# !pip install --upgrade google-cloud-speech

# GOOGLE_APPLICATION_CREDENTIALS='/content/drive/My Drive/sounds/calhacks6-ef3b40476144-tts.json'
# SYNTH_FILE='/content/drive/My Drive/sounds/synth.wav'
# IMPEDIMENT_FILE='/content/drive/My Drive/sounds/impediment.wav'

# !pip install pydub
def stereo_to_mono(audio_file_name):
    sound = AudioSegment.from_wav(audio_file_name)
    sound = sound.set_channels(1)
    sound.export(audio_file_name, format="wav")


# Instantiates a client
# speechClient = speech.SpeechClient.from_service_account_json(GOOGLE_APPLICATION_CREDENTIALS)
speechClient = speech.SpeechClient()

def get_text_from_speech(file_path, client):
  # The name of the audio file to transcribe
  file_name = file_path
  clean_soundfile(file_path)

  # Loads the audio into memory
  with io.open(file_name, 'rb') as audio_file:
      content = audio_file.read()
      audio = types.RecognitionAudio(content=content)

  # print(audio)
  config = types.RecognitionConfig(
      encoding=enums.RecognitionConfig.AudioEncoding.LINEAR16,
      sample_rate_hertz=44100,
      language_code='en-US',
      model='default')

  # Detects speech in the audio file
  response = client.recognize(config, audio)
  print(f'response={response}')

  for result in response.results:
      transcript=result.alternatives[0].transcript
      print('Transcript: {}'.format(result.alternatives[0].transcript))

  return transcript

# print(get_text_from_speech(SYNTH_FILE, speechClient))
# print(get_text_from_speech(IMPEDIMENT_FILE, speechClient))

# Audio(SYNTH_FILE, autoplay=True)

# !pip install --upgrade google-cloud-texttospeech

"""Synthesizes speech from the input string of text or ssml.

Note: ssml must be well-formed according to:
    https://www.w3.org/TR/speech-synthesis/
"""

# Instantiates a client
# textToSpeechClient = texttospeech.TextToSpeechClient.from_service_account_json(filename=GOOGLE_APPLICATION_CREDENTIALS)
textToSpeechClient = texttospeech.TextToSpeechClient()

def get_speech_from_text(text, client):
  # Set the text input to be synthesized
  synthesis_input = texttospeech.types.SynthesisInput(text=text)

  # Build the voice request, select the language code ("en-US") and the ssml
  # voice gender ("neutral")
  voice = texttospeech.types.VoiceSelectionParams(
      language_code='en-US',
      ssml_gender=texttospeech.enums.SsmlVoiceGender.NEUTRAL)

  # Select the type of audio file you want returned
  audio_config = texttospeech.types.AudioConfig(
      audio_encoding=texttospeech.enums.AudioEncoding.MP3)

  # Perform the text-to-speech request on the text input with the selected
  # voice parameters and audio file type
  response = client.synthesize_speech(synthesis_input, voice, audio_config)

  # The response's audio_content is binary.
  with open('output.mp3', 'wb') as out:
      # Write the response to the output file.
      out.write(response.audio_content)
      print('Audio content written to file "output.mp3"')
      
  return 'output.mp3'

def get_audio_file(transcript):
  return get_speech_from_text(transcript, textToSpeechClient)
   
# Audio(file, autoplay=False)

# def get_impediment_score(input_file):
#   synth_file = get_audio_file()
#   impediments = []
#   for audio_file in [input_file, './output.mp3']:
#     y, sr = librosa.load(audio_file)
#     impediments.append(librosa.feature.chroma_stft(y, sr))

# np.mean(impediments[0]) - np.mean(impediments[1])
clients = {"textToSpeechClient" : textToSpeechClient, "speechClient": speechClient}

def get_mean_impediment_diff(fileData, clients=clients):
  with open('input.mp3', 'wb') as out:
      # Write the response to the output file.
      out.write(fileData)
      print('Audio content written to file "input.mp3"')
  
  original_file='./input.mp3'
  text = get_text_from_speech(original_file, clients['speechClient'])
  synth_file = get_speech_from_text(text, clients['textToSpeechClient'])
  impediments = []
  for audio_file in [original_file, synth_file]:
    y, sr = librosa.load(audio_file)
    impediments.append(librosa.feature.chroma_stft(y, sr))

  return np.mean(impediments[0]) - np.mean(impediments[1])
# print(get_mean_impediment_diff(IMPEDIMENT_FILE, clients))
# print(get_mean_impediment_diff(SYNTH_FILE, clients))

